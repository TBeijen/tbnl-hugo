---
title: 'KubeCon + CloudNativeCon 2019 Barcelona'
author: Tibo Beijen
date: 2019-06-09T10:00:00+01:00
url: /2019/02/01/kubecon-cloudnativecon-2019/
categories:
  - articles
tags:
  - DevOps
  - Kubernetes
  - CNCF
  - Conference
description: "Looking back at KubeCon + CloudNativeCon 2019 (2 weeks after the fact)."

---
The end of May 2019 saw 7700 people, myself included, visit Barcelona to attend ['KubeCon + CloudNativeCon Europe 2019'](https://kccnceu19.sched.com/). That's a *lot* of people and a significant increase of previous editions (Copenhagen 2018: 4300, Berlin 2017: 1500, according to [this Dutch source](https://www.computable.nl/artikel/nieuws/cloud-computing/6668953/250449/kubecon-conferentie-komt-in-2020-naar-amsterdam.html)). Next year's edition will be 'around the corner' in Amsterdam and is projected to attract at least 10000 visitors. This is quite telling of the increased adoption of Kubernetes and all associated technologies over the past years.

So, what's the current state of the Cloud-Native ecosystem?

## Impressions

Well, for starters, talking about 'current state' can't reasonably done without specifying a pretty concise timeframe, as a lot is moving at a very fast pace. Two quotes mentioned in the keynotes summarize the eco-system quite well in my opinion:

> "A platform to build platforms"

In lego-terms, you could see Kubernetes as a base plate. You pick your color (GKE, AKS, EKS, Kops, ...) and then will need to add things on top, as just a baseplate is of very little use. What to build on top is up to you. What lego do you already have? What do you want it to do? New Lego boxes will be launched regularly but you won't be able to buy them all. Do you need a new one? Does it nicely supplement or replace what you already have?

> "Culture eats strategy for breakfast"

A thriving community that grows, engages, participates, contributes, will propel innovation at a more rapid pace than any (single vendor) strategy would be able to accomplish. I suppose that's what it boils down to. Meaning it's not a bad thing per se that Istio and Linkerd both offer Service Mesh capabilities. Or that there's many ways to template manifests. Or that Helm v2 installs a cluster component (Tiller) that does not go too well with the later introduced RBAC.

## Some topics

### Service Mesh

Service meshes are a hot topic, and Kubecon witnessed the [introduction](https://cloudblogs.microsoft.com/opensource/2019/05/21/service-mesh-interface-smi-release/) of the [Service Mesh Interface (SMI)](https://smi-spec.io/). 

On the other hand, with [Istio](https://istio.io) being around for quite a while ([v1 having been announced](https://istio.io/blog/2018/announcing-1.0/) in July 2018), I was surprised by the small amount of hands going up to the question "who is using a service mesh in production?" at the '[service mesh breakfast](https://kccnceu19.sched.com/event/NxQW/the-new-stack-pancake-breakfast-sponsored-by-vmware)'. It seems that for a lot of people the benefit vs. complexity trade-off is not there yet. Or service meshes aren't on the top of the wish list. Or people are waiting for more reports from early adopters before wetting their feet themselves. Or a combination of all of the above. Might just be my perspective though...

### Loki

Already introduced at KubeCon Seattle, [Loki](https://grafana.com/loki) looked very interesting. The ability to handle the high volume of logs in a light-weight manner, integrating well with Grafana and the auto-discovery and labeling of Prometheus, simply sounds very good. Still beta though.

### Helm v3

[Helm 3 alpha](https://v3.helm.sh/) is out now, and the biggest change to v2 is the removal of Tiller from the server. Now keeping Tiller out of a cluster [already was possible](https://rimusz.net/tillerless-helm) but not it comes out of the box without needing to handle Tiller anymore. Another notable change is storing of release info into the same namespace of the release itself, allowing same-named releases to exist in different namespaces. For more info it's worth checking out the 'Charting our future' series on [the Helm blog](https://helm.sh/blog/helm-3-preview-pt7/).

### Virtual-kubelet

I was aware of [Virtual Kubelet](https://virtual-kubelet.io/), however missed the v1 anouncement (so many tracks).  Being able to run Kubernetes without having to bother about any infrastructure, allowing inifite scale while paying for actual use, would have great value. 

However, it looks like the "v1 - We're ready" mainly applies to Azure (and perhaps other providers) as [the warning in the AWS Fargate docs](https://github.com/virtual-kubelet/virtual-kubelet/tree/master/providers/aws#aws-fargate-virtual-kubelet-provider) is pretty clear. [This Github issue](https://github.com/virtual-kubelet/virtual-kubelet/issues/185#issuecomment-452542691) very well illustrates the type of implementation details hidden under the virtual-kubelet abstraction.

### Cluster API


### Storage



----




Thoughts

* Many tracks, chosing and inevatibly missing interesting talks

Heard in a podcast. Kubernetes should become boring.


